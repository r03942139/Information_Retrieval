{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "430005499_hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtC79cgeSLND",
        "colab_type": "text"
      },
      "source": [
        "#### Information Retrieval :: Recommendations with Implicit Feedbacks\n",
        "\n",
        "\n",
        "# Recommendations with Implicit Feedbacks: Rating Prediction and Top-K Item Recommendation\n",
        "\n",
        "\n",
        "\n",
        "*Goals of this tutorial:* Understand matrix factorization (MF) using explicit feedback and Bayesian Personalized Ranking (BPR) using implicit feedback for recommendation. Explore different methods for two real-world recommendation senarios: rating prediction and top-K item recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIZsFBf5SLNH",
        "colab_type": "text"
      },
      "source": [
        "# Part 1. Matrix Factorization for Rating Prediction \n",
        "\n",
        "In some platforms, such as MovieLens, users express their preference on items using explict feedback like ratings.\n",
        "\n",
        "In this part, a matrix factorization will be implemented to predict ratings on MovieLens data. After removing users who left less than 20 ratings and movies with less than 20 ratings, the provided dataset has only ~1,200 items and ~500 users. You can also check the title and genres of each movie in *movies_info.csv*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv5tBOuFSLNJ",
        "colab_type": "text"
      },
      "source": [
        "## Part 1a: Load the Data\n",
        "\n",
        "In the Movie dataset, there are about 65,000 ratings in total. We split the rating data into two sets. You will train with 70% of the data (in *train_movie.csv*) and test on the remaining 30% of data (in *test_movie.csv*). Each of train and test files has lines having this format: UserID, MovieID, Rating. \n",
        "\n",
        "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1oqUyhpSLNK",
        "colab_type": "code",
        "outputId": "5b7b3787-71b3-420d-d85d-a2d825231a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# load the data, then print out the number of\n",
        "# movies and users in each of train and test sets.\n",
        "# Your Code Here...\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv(\"train_movie.csv\", sep=',', header=None, skiprows=1)\n",
        "train_df.columns = ['user', 'movie', 'rating']\n",
        "test_df = pd.read_csv('test_movie.csv', sep=',', header=None, skiprows=1)\n",
        "test_df.columns = ['user', 'movie', 'rating']\n",
        "\n",
        "print (\"Number of unique users in train set is \", len(train_df.user.unique()) )\n",
        "print (\"Number of unique movie in train set is \", len(train_df.movie.unique()) )\n",
        "print (\"Number of unique users in test set is \", len(test_df.user.unique()) )\n",
        "print (\"Number of unique movie in test set is \", len(test_df.movie.unique()) )\n",
        "print (train_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique users in train set is  541\n",
            "Number of unique movie in train set is  1211\n",
            "Number of unique users in test set is  541\n",
            "Number of unique movie in test set is  1211\n",
            "       user  movie  rating\n",
            "0         0      4       4\n",
            "1         0     23       5\n",
            "2         0     25       5\n",
            "3         0     31       3\n",
            "4         0     33       5\n",
            "...     ...    ...     ...\n",
            "45277   540   1202       3\n",
            "45278   540   1204       3\n",
            "45279   540   1205       4\n",
            "45280   540   1206       4\n",
            "45281   540   1207       4\n",
            "\n",
            "[45282 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Di6E6_SLNP",
        "colab_type": "text"
      },
      "source": [
        "## Part 1b: Matrix Factorization\n",
        "\n",
        "In this part, we will implement a matrix factorization mechanism for recommendations. There are different methods to obtain the latent factor matrices **P** and **Q**, like gradient descent, Alternating Least Squares (ALS), and so on. Pick one of them and implement your MF model.\n",
        "\n",
        "The metrics MAE and RMSE of implemented MF model will be shown on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O9dnrnEpUTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cosine Similarity of User–User Collaborative Filtering\n",
        "import math\n",
        "from functools import reduce\n",
        "\n",
        "rating_matrix = train_df.pivot_table(index='user', columns='movie', values='rating').fillna(0).to_numpy()\n",
        "actual_rating = test_df.pivot_table(index='user', columns='movie', values='rating').fillna(0).to_numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUfiR5XZipT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# The class of MF\n",
        "class MF:\n",
        "    def __init__(self, train, test, hidden=40, epoch=10, lr=0.01, bias=False):\n",
        "        self.bias = bias\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.mask = 1.0 * (train > 0)\n",
        "        self.testmask = 1.0 * (test > 0)\n",
        "        self.num_train = np.sum(self.mask)\n",
        "        self.num_test  = np.sum(self.testmask)\n",
        "        self.num_user, self.num_item = train.shape\n",
        "        self.hidden = hidden\n",
        "\n",
        "        if self.bias:\n",
        "            self.global_bias = np.sum(train) / np.sum(self.mask)\n",
        "            self.user_bias = np.sum((train - self.global_bias) * self.mask, axis=1) / np.sum(self.mask, axis=1)\n",
        "            self.item_bias = np.sum((train - self.global_bias) * self.mask, axis=0) / np.sum(self.mask, axis=0)\n",
        "        else:\n",
        "            self.user_bias = np.zeros(self.num_user)\n",
        "            self.item_bias = np.zeros(self.num_item)\n",
        "            self.global_bias = 0.0\n",
        "\n",
        "        self.epoch = epoch\n",
        "\n",
        "        self.sample_row, self.sample_col = self.train.nonzero()\n",
        "        self.n_samples = len(self.sample_row)\n",
        "        self.lr = lr\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # initialize\n",
        "        self.P = np.random.random((self.num_user, self.hidden))\n",
        "        self.Q = np.random.random((self.num_item, self.hidden))\n",
        "\n",
        "        for itr in range(self.epoch):\n",
        "            self.training_indices = np.arange(self.n_samples)\n",
        "            np.random.shuffle(self.training_indices)\n",
        "            for idx in self.training_indices:\n",
        "                u = self.sample_row[idx]\n",
        "                i = self.sample_col[idx]\n",
        "                rating = self.train[u, i]\n",
        "                e = rating - self.P[u, :].dot(self.Q[i, :].T) - self.global_bias - self.item_bias[i] - self.user_bias[u]\n",
        "                self.P[u, :] += self.lr * (e * self.Q[i, :])\n",
        "                self.Q[i, :] += self.lr * (e * self.P[u, :])\n",
        "\n",
        "            Rec = self.P.dot(self.Q.T) + self.global_bias + self.item_bias + self.user_bias.reshape((-1, 1))\n",
        "            error = np.sqrt(np.sum(np.square(Rec * self.mask - self.train)) / self.num_train)\n",
        "            print('iters={0}, RMSE={1}'.format(itr, error))\n",
        "        \n",
        "        # Calculate the RMSE, MAE between predicted and trained\n",
        "        self.Rec_mf = (self.P.dot(self.Q.T) + self.global_bias + self.item_bias + self.user_bias.reshape((-1, 1)))\n",
        "        RMSE = np.sqrt(np.sum(np.square(self.Rec_mf * self.testmask - self.test)) / self.num_test)\n",
        "        MAE = np.sum(np.abs(self.Rec_mf * self.testmask - self.test)) / self.num_test\n",
        "        return self.P, self.Q, self.Rec_mf, RMSE, MAE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhK75hpyoZHt",
        "colab_type": "code",
        "outputId": "1b3dfe1a-420f-4458-f6b4-e0ee87fe0445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# train an MF model without bias, print the RMSE for each epoch\n",
        "mf = MF(rating_matrix, actual_rating, hidden=40, epoch=20, lr=0.01, bias=False)\n",
        "P_mf, Q_mf, Rec_mf, RMSE, MAE = mf.fit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters=0, RMSE=1.0180046342552176\n",
            "iters=1, RMSE=0.8711010328503331\n",
            "iters=2, RMSE=0.7989419214059273\n",
            "iters=3, RMSE=0.7573126422744588\n",
            "iters=4, RMSE=0.7260011624308345\n",
            "iters=5, RMSE=0.6970840164352864\n",
            "iters=6, RMSE=0.672424358607756\n",
            "iters=7, RMSE=0.6529578252431153\n",
            "iters=8, RMSE=0.6284094327191848\n",
            "iters=9, RMSE=0.6085078032776038\n",
            "iters=10, RMSE=0.5881611040385031\n",
            "iters=11, RMSE=0.5699243416449129\n",
            "iters=12, RMSE=0.5537492815058511\n",
            "iters=13, RMSE=0.5367899067026205\n",
            "iters=14, RMSE=0.5213113521035709\n",
            "iters=15, RMSE=0.5056982253345237\n",
            "iters=16, RMSE=0.4916547901656696\n",
            "iters=17, RMSE=0.4786446660081976\n",
            "iters=18, RMSE=0.46677792052145994\n",
            "iters=19, RMSE=0.45314279244986727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDixCXx6XMmo",
        "colab_type": "code",
        "outputId": "891d79d8-df32-48d6-8f1d-1cff1aa8c423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# P_mf, Q_mf, Rec_mf = mf.fit()\n",
        "print(\"RMSE =\", str(RMSE))\n",
        "print(\"MAE =\", str(MAE))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE = 1.020489876333291\n",
            "MAE = 0.7957720435290563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl1yL1gRSLNZ",
        "colab_type": "text"
      },
      "source": [
        "Which method did you use to obtain **P** and **Q**? What are the advantages and disadvantages of the method you pick? *provide a brief (1-2 paragraph) discussion based on these questions.*\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhlNRcOQSLNa",
        "colab_type": "text"
      },
      "source": [
        "I find **P** and **Q** by using gradient descent. AThe biggest advantage of gradient descent is that it can be fast on computations, because it can be done on matrix calculations on each iteration.\n",
        "<br/>The disadvantage of gradient descent is that it takes lots of iterations to converge when the learning rate is not set properly. Also, it requires a large memory to store Intermediate parameters. Furthermore, if the matrix is sparse, this procedure may not be powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6XpFqCSLNb",
        "colab_type": "text"
      },
      "source": [
        "## Part 1c: Improve MF\n",
        "\n",
        "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach. Hints: You may consider using the title or genres information, trying other algorithms, designing a hybrid system or considering a neighborhood like this paper [Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf). *You can do anything you like to improve MAE and RMSE.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXK1u3_XzUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# The class of MF\n",
        "class ImproveMF:\n",
        "    def __init__(self, train, test, hidden=40, epoch=10, lr=0.01, bias=False, lambda1 = 0.02, lambda2 = 0.03):\n",
        "        self.bias = bias\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.mask = 1.0 * (train > 0)\n",
        "        self.testmask = 1.0 * (test > 0)\n",
        "        self.num_train = np.sum(self.mask)\n",
        "        self.num_test  = np.sum(self.testmask)\n",
        "        self.num_user, self.num_item = train.shape\n",
        "        self.hidden = hidden\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "\n",
        "        if self.bias:\n",
        "            self.global_bias = np.sum(train) / np.sum(self.mask)\n",
        "            self.user_bias = np.sum((train - self.global_bias) * self.mask, axis=1) / np.sum(self.mask, axis=1)\n",
        "            self.item_bias = np.sum((train - self.global_bias) * self.mask, axis=0) / np.sum(self.mask, axis=0)\n",
        "        else:\n",
        "            self.user_bias = np.zeros(self.num_user)\n",
        "            self.item_bias = np.zeros(self.num_item)\n",
        "            self.global_bias = 0.0\n",
        "\n",
        "        self.epoch = epoch\n",
        "\n",
        "        self.sample_row, self.sample_col = self.train.nonzero()\n",
        "        self.n_samples = len(self.sample_row)\n",
        "        self.lr = lr\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # initialize\n",
        "        self.P = np.random.random((self.num_user, self.hidden))\n",
        "        self.Q = np.random.random((self.num_item, self.hidden))\n",
        "        \n",
        "        for itr in range(self.epoch):\n",
        "            self.training_indices = np.arange(self.n_samples)\n",
        "            np.random.shuffle(self.training_indices)\n",
        "            for idx in self.training_indices:\n",
        "                u = self.sample_row[idx]\n",
        "                i = self.sample_col[idx]\n",
        "                rating = self.train[u, i]\n",
        "                e = rating - self.P[u, :].dot(self.Q[i, :].T) - self.global_bias - self.item_bias[i] - self.user_bias[u]\n",
        "                e = e + self.lambda1 * pow(self.P[u, :], 2) + self.lambda2 * (pow(self.Q[i, :].T, 2) + pow(self.item_bias[i], 2) + pow(self.user_bias[u], 2))\n",
        "                self.P[u, :] += self.lr * (e * self.Q[i, :])\n",
        "                self.Q[i, :] += self.lr * (e * self.P[u, :])\n",
        "\n",
        "            Rec = self.P.dot(self.Q.T) + self.global_bias + self.item_bias + self.user_bias.reshape((-1, 1))\n",
        "            error = np.sqrt(np.sum(np.square(Rec * self.mask - self.train)) / self.num_train)\n",
        "            print('iters={0}, RMSE={1}'.format(itr, error))\n",
        "        \n",
        "        # Calculate the RMSE, MAE between predicted and trained\n",
        "        self.Rec_mf = (self.P.dot(self.Q.T) + self.global_bias + self.item_bias + self.user_bias.reshape((-1, 1)))\n",
        "        RMSE = np.sqrt(np.sum(np.square(self.Rec_mf * self.testmask - self.test)) / self.num_test)\n",
        "        MAE = np.sum(np.abs(self.Rec_mf * self.testmask - self.test)) / self.num_test\n",
        "        return self.P, self.Q, self.Rec_mf, RMSE, MAE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnA7MTlpSLNd",
        "colab_type": "code",
        "outputId": "c5d2aaa8-7b57-46e8-ee98-1db550dfc818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# Your Code Here...\n",
        "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
        "# train an MF model without bias, print the RMSE for each epoch\n",
        "mf = ImproveMF(rating_matrix, actual_rating, hidden=3, epoch=30, lr=0.002, bias=False, lambda1=0.02, lambda2=0.03)\n",
        "P_mf, Q_mf, Rec_mf, RMSE, MAE = mf.fit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters=0, RMSE=2.0523402425812525\n",
            "iters=1, RMSE=1.5148829460013733\n",
            "iters=2, RMSE=1.2644762249585697\n",
            "iters=3, RMSE=1.1219333004882837\n",
            "iters=4, RMSE=1.0308865750839322\n",
            "iters=5, RMSE=0.9698184970130348\n",
            "iters=6, RMSE=0.9277808498585163\n",
            "iters=7, RMSE=0.8978907544551247\n",
            "iters=8, RMSE=0.8765143164823072\n",
            "iters=9, RMSE=0.8605543230385659\n",
            "iters=10, RMSE=0.8486017248564154\n",
            "iters=11, RMSE=0.8395262857172707\n",
            "iters=12, RMSE=0.8324515484463253\n",
            "iters=13, RMSE=0.8270979450143148\n",
            "iters=14, RMSE=0.8227810965941166\n",
            "iters=15, RMSE=0.819735293295987\n",
            "iters=16, RMSE=0.8168638438765574\n",
            "iters=17, RMSE=0.8146697740013251\n",
            "iters=18, RMSE=0.8127362800426613\n",
            "iters=19, RMSE=0.8112916281627744\n",
            "iters=20, RMSE=0.8101892981434299\n",
            "iters=21, RMSE=0.8089792956188272\n",
            "iters=22, RMSE=0.8080264557633803\n",
            "iters=23, RMSE=0.8071712839285646\n",
            "iters=24, RMSE=0.8064694312358457\n",
            "iters=25, RMSE=0.8058901614135167\n",
            "iters=26, RMSE=0.8051688408493616\n",
            "iters=27, RMSE=0.804685528976083\n",
            "iters=28, RMSE=0.8040220852254698\n",
            "iters=29, RMSE=0.8038674390917876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_07aDk6OXR5L",
        "colab_type": "code",
        "outputId": "ab0d207e-f9e5-4980-f878-525cd0de949c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# P_mf, Q_mf, Rec_mf = mf.fit()\n",
        "print(\"RMSE =\", str(RMSE))\n",
        "print(\"MAE =\", str(MAE))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE = 0.8490170234446313\n",
            "MAE = 0.6607409045407778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwoWKqYmSLNk",
        "colab_type": "text"
      },
      "source": [
        "Please explain what you do to improve the recommendation in 1-2 paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xab4CNkySLNo",
        "colab_type": "text"
      },
      "source": [
        "Compared with Part1b, I introduced regularization to avoid overfitting by adding different lambda parameters for ***P*** and ***Q***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpllHEZzSLNq",
        "colab_type": "text"
      },
      "source": [
        "# Part 2. Bayesian Personalized Ranking (BPR) for Top-K Item Recommendation\n",
        "\n",
        "Compared with rating prediction in part 1, a more popular scenario recently is personalized top-K item ranking for each user based on the user's implicit feedback. Examples include ranking videos on YouTube and ranking products on Aamzon. In practice, users tend to provide implicit feedback (e.g., the user clicked a product URL on Amazon or played a video on YouTube) rather than explicit feedback (e.g., ratings or reviews) in most cases.\n",
        "\n",
        "In this part, you will experiment with Bayesian Personalized Ranking (BPR) to rank items on a [Spotify Playlist Recommendation Dataset](http://people.tamu.edu/~yunhe/pubs/AttListCIKM2019.pdf). If a user ever followed a playlist, this interaction is treated as an implicit feedback. In our sampled dataset, there are ~10,000 users and ~7,000 playlists.\n",
        "\n",
        "BPR can generate scores of items for each user. You should rank all items based on the scores for each user and evaluate the ranking performance.\n",
        "\n",
        "For example, if user 0 has two interacted playlists 23, 78 in test.txt. If the top-10 playlists for user 0 returned by BPR is [12,45,78,34,23,90,134,33,46,9], then the precision@10 for user 0 is 0.2 because the two playlists in test.txt are recommended in top-10: 2/10=0.2. Please report NDCG@10 in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4yd_QvSSLNs",
        "colab_type": "text"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "Please download the dataset from Piazza. There are about 90,000 interactions in total, which are split into training.txt, validation.txt and text.txt. You will train on train.txt, tune hyperparameters on validation.txt and report final result on test.txt in terms of NDCG@10. \n",
        "\n",
        "Each of the train and test files has lines having this format: UserID, PlaylistID, 1.0. \n",
        "\n",
        "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weaebe54SLNt",
        "colab_type": "code",
        "outputId": "c9f3aee3-6eb1-45cc-d8fb-12901df627b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# load the data, then print out the number of\n",
        "# playlists and users in each of train and test sets.\n",
        "# Your Code Here...\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('train.txt', sep=\"\\t\", header=None)\n",
        "train_df.columns = [\"userid\", \"playlistid\", \"one\"]\n",
        "validation_df = pd.read_csv('validation.txt', sep=\"\\t\", header=None)\n",
        "validation_df.columns = [\"userid\", \"playlistid\", \"one\"]\n",
        "test_df = pd.read_csv('validation.txt', sep=\"\\t\", header=None)\n",
        "test_df.columns = [\"userid\", \"playlistid\", \"one\"]\n",
        "\n",
        "print (\"Number of unique users in train set is \", len(train_df.userid.unique()) )\n",
        "print (\"Number of unique movie in train set is \", len(train_df.playlistid.unique()) )\n",
        "print (\"Number of unique users in validate set is \", len(validation_df.userid.unique()) )\n",
        "print (\"Number of unique movie in validate set is \", len(validation_df.playlistid.unique()) )\n",
        "print (\"Number of unique users in test set is \", len(test_df.userid.unique()) )\n",
        "print (\"Number of unique movie in test set is \", len(test_df.playlistid.unique()) )\n",
        "\n",
        "num_users = len(train_df.userid.unique())\n",
        "num_items = len(train_df.playlistid.unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique users in train set is  10183\n",
            "Number of unique movie in train set is  7787\n",
            "Number of unique users in validate set is  5895\n",
            "Number of unique movie in validate set is  3674\n",
            "Number of unique users in test set is  5895\n",
            "Number of unique movie in test set is  3674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hykoy9b6SLNy",
        "colab_type": "text"
      },
      "source": [
        "## BPR by Using Package\n",
        "\n",
        "Compared with MF, BPR is more complicated to implement. In this part, you can use a BPR package to experiment with top-K item recommendation. Some good packages include https://github.com/benfred/implicit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGK5gjul_eqP",
        "colab_type": "code",
        "outputId": "5dbacc0d-3336-4d25-8a26-c5e2d0918e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "#!pip3 install -Iv implicit==0.4.0\n",
        "#!pip3 uninstall implicit\n",
        "!pip install lightfm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightfm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/8e/5485ac5a8616abe1c673d1e033e2f232b4319ab95424b42499fabff2257f/lightfm-1.15.tar.gz (302kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightfm) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from lightfm) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->lightfm) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->lightfm) (2.8)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.15-cp36-cp36m-linux_x86_64.whl size=707643 sha256=f3325809d9ef48efe7440f4dcf2033ef79f37226573c9a80dee456be37e36412\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/bb/ac/188385a5da6627956be5d9663928483b36da576149ab5b8f79\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pub_EWmHSLN1",
        "colab_type": "code",
        "outputId": "3d00bc82-9af0-4d7e-9c68-aafb5c0b9a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "## your code to call other BPR packages for top-K recommendation.\n",
        "# Report average NDCG@10 for all users on test.txt\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import precision_at_k\n",
        "from lightfm.evaluation import auc_score\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "def GenerateUserItemDict(TrainFile):\n",
        "    fp = open(TrainFile)\n",
        "    lines = fp.readlines()\n",
        "    useritemTrainDict = {}\n",
        "    for line in lines:\n",
        "        lineStr = line.strip().split('\\t')\n",
        "        userId = int(lineStr[0])\n",
        "        itemId = int(lineStr[1])\n",
        "        if userId not in useritemTrainDict:\n",
        "            useritemTrainDict[userId] = [itemId]\n",
        "        else:\n",
        "            useritemTrainDict[userId].append(itemId)\n",
        "    fp.close()\n",
        "    train = sp.dok_matrix((num_users, num_items), dtype=np.int8)\n",
        "    for user_id, movie_ids in useritemTrainDict.items():\n",
        "        train[user_id, movie_ids] = 1\n",
        "    train = train.tocsr() # (num_users, num_items) sparse matrix \n",
        "    return useritemTrainDict, train\n",
        "\n",
        "def Model_Predict(train, test, useritemTestDict, learningrate, no_component):\n",
        "    model = LightFM(learning_rate=learningrate, loss='bpr', no_components=no_component)\n",
        "    model.fit(train, epochs=30)\n",
        "    #train_precision = precision_at_k(model, train, k=10).mean()\n",
        "    #test_precision  = precision_at_k(model, test, k=10).mean()\n",
        "    train_auc = auc_score(model, train).mean()\n",
        "    test_auc = auc_score(model, test).mean()\n",
        "    #print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
        "    print('AUC: train %.2f, validate %.2f.' % (train_auc, test_auc))\n",
        "\n",
        "    ## Calculate NDCG@10\n",
        "    n_users, n_items = test.shape\n",
        "    ndcg10 = 0.0\n",
        "    for user_id in useritemTestDict:\n",
        "        scores = model.predict(user_id, np.arange(n_items))\n",
        "        top10_items = np.argsort(-scores)[:10]\n",
        "\n",
        "        dcgPerUser = 0\n",
        "        idcgPerUser = 0\n",
        "        rank = 0\n",
        "        groundtruthLength = len(useritemTestDict[user_id])\n",
        "        if groundtruthLength >= 10:\n",
        "            groundtruthLength = 10\n",
        "        for index in range(groundtruthLength):\n",
        "            rank = rank + 1\n",
        "            idcgPerUser += (1 / float(math.log(rank + 1, 2)))\n",
        "        rank = 0\n",
        "        for itemIndex in top10_items:\n",
        "            if itemIndex in useritemTestDict[user_id]:\n",
        "                position = rank + 1\n",
        "                dcgPerUser += 1 / float(math.log(position + 1, 2))\n",
        "            rank = rank + 1\n",
        "        ndcgPerUser = float(dcgPerUser) / float(idcgPerUser)\n",
        "        ndcg10 = ndcg10 + ndcgPerUser\n",
        "    print('NDCG@10: validate' + str(float(ndcg10)/float(len(useritemTestDict))))\n",
        "    return model, float(ndcg10)/float(n_users)\n",
        "\n",
        "useritemTrainDict, train = GenerateUserItemDict(\"train.txt\")\n",
        "useritemValiDict, validate = GenerateUserItemDict(\"validation.txt\")\n",
        "print (\"Model1\")\n",
        "model1, ndcg1 = Model_Predict (train, validate, useritemValiDict, 0.05, 10)\n",
        "print (\"Model2\")\n",
        "model2, ndcg2 = Model_Predict (train, validate, useritemValiDict, 0.05, 50)\n",
        "print (\"Model3\")\n",
        "model3, ndcg3 = Model_Predict (train, validate, useritemValiDict, 0.05, 40)\n",
        "print (\"Model4\")\n",
        "model4, ndcg4 = Model_Predict (train, validate, useritemValiDict, 0.05, 45)\n",
        "print (\"Model5\")\n",
        "model5, ndcg5 = Model_Predict (train, validate, useritemValiDict, 0.05, 35)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model1\n",
            "AUC: train 0.92, validate 0.71.\n",
            "NDCG@10: validate0.06587136507731556\n",
            "Model2\n",
            "AUC: train 0.95, validate 0.72.\n",
            "NDCG@10: validate0.07501440439036912\n",
            "Model3\n",
            "AUC: train 0.95, validate 0.72.\n",
            "NDCG@10: validate0.0765644704173029\n",
            "Model4\n",
            "AUC: train 0.95, validate 0.72.\n",
            "NDCG@10: validate0.07505338460275243\n",
            "Model5\n",
            "AUC: train 0.95, validate 0.72.\n",
            "NDCG@10: validate0.07517939236564417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz-BJXN7PybN",
        "colab_type": "code",
        "outputId": "c9e31191-c2e8-4437-f40a-f832683b8841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#### Test #### (Using model3 since it has the best NDCG@10 on validate.txt)\n",
        "useritemTestDict, test = GenerateUserItemDict(\"test.txt\")\n",
        "## Calculate NDCG@10\n",
        "n_users, n_items = test.shape\n",
        "ndcg10 = 0.0\n",
        "for user_id in useritemTestDict:\n",
        "    scores = model3.predict(user_id, np.arange(n_items))\n",
        "    top10_items = np.argsort(-scores)[:10]\n",
        "\n",
        "    dcgPerUser = 0\n",
        "    idcgPerUser = 0\n",
        "    rank = 0\n",
        "    if len(useritemTestDict[user_id]) !=0:\n",
        "        groundtruthLength = len(useritemTestDict[user_id])\n",
        "        if groundtruthLength >= 10:\n",
        "            groundtruthLength = 10\n",
        "        for index in range(groundtruthLength):\n",
        "            rank = rank + 1\n",
        "            idcgPerUser += 1 / float(math.log(rank + 1, 2))\n",
        "        rank = 0\n",
        "        for itemIndex in top10_items:\n",
        "            if itemIndex in useritemTestDict[user_id]:\n",
        "                position = rank + 1\n",
        "                dcgPerUser += 1 / float(math.log(position + 1, 2))\n",
        "            rank = rank + 1\n",
        "        ndcgPerUser = float(dcgPerUser) / float(idcgPerUser)\n",
        "        ndcg10 = ndcg10 + ndcgPerUser\n",
        "print('NDCG@10: test ' + str(float(ndcg10)/float(len(useritemTestDict))))\n",
        "\n",
        "test_auc = auc_score(model3, test).mean()\n",
        "print('AUC: test ' + str(test_auc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDCG@10: test 0.07449423713979982\n",
            "AUC: test 0.71989447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339H-w46SLN8",
        "colab_type": "text"
      },
      "source": [
        "## Collaboration declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9UcckKFSLN9",
        "colab_type": "text"
      },
      "source": [
        "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_tjO8qsQ4Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install LightFM\n",
        "#https://making.lyst.com/lightfm/docs/lightfm.html\n",
        "#https://github.com/lyst/lightfm/blob/master/examples/quickstart/quickstart.ipynb\n",
        "# Using the lightfm package to implement the BPR"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}