{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": " Word_Embeddings_for_Information_Retrieval_and_Query_Expansion.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgQz_sXb08wt",
        "colab_type": "text"
      },
      "source": [
        "#### Information Retrieval :: Word Embeddings for Information Retrieval and Query Expansion\n",
        "\n",
        "\n",
        "# Word Embeddings for Information Retrieval and Query Expansion\n",
        "\n",
        "\n",
        "*Goals of this tutorial:* In this tutorial the information retrieval engine will be improved by: (i) directly match the query and the document in the latent semantic space of word embeddings; (ii) expand the original query via word embeddings.\n",
        "\n",
        "<br>*Reference:* Christopher D. Manning publication \"Introduction to Information Retrieval\" Chap 9: Relevance feedback and query expansion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s69g1Pb08w2",
        "colab_type": "text"
      },
      "source": [
        "## Part 0. Dataset and Parsing (The same as Modeling_and_Ranking_Texts part)\n",
        "\n",
        "The dataset is collected from Quizlet (https://quizlet.com), a website where users can generated their own flashcards. Each flashcard generated by a user is made up of an entity on the front and a definition describing or explaining the entity correspondingly on the back. We treat entities on each flashcard's front as the queries and the definitions on the back of flashcards as the documents. Definitions (documents) are relevant to an entity (query) if the definitions are from the back of the entity's flashcard; otherwise definitions are not relevant. **In this homework, queries and entities are interchangeable as well as documents and definitions.**\n",
        "\n",
        "The format of the dataset is like this:\n",
        "\n",
        "**query \\t document id \\t document**\n",
        "\n",
        "Examples:\n",
        "\n",
        "decision tree\t\\t 27946 \\t\tshow complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\n",
        "\n",
        "where \"decision tree\" is the entity in the front of a flashcard and \"show complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\" is the definition on the flashcard's back and \"27946\" is the id of the definition. Naturally, this document is relevant to the query.\n",
        "\n",
        "false positive rate\t\\t 686\t\\t fall-out; probability of a false alarm\n",
        "\n",
        "where document 686 is not relevant to query \"decision tree\" because the entity of \"fall-out; probability of a false alarm\" is \"false positive rate\".\n",
        "\n",
        "For parsing this dataset, you could also just copy your code from homework 1 to complete the following tasks:\n",
        "* Tokenize documents (definitions) using **whitespaces and punctuations as delimiters**.\n",
        "* Remove stop words: use nltk stop words list (from nltk.corpus import stopwords)\n",
        "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
        "* Remove any other strings that you think are less informative or nosiy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcng_6MEhouR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install nltk\n",
        "# Build document repo\n",
        "from collections import OrderedDict\n",
        "with open('homework_1_data.txt', encoding='utf8') as f:\n",
        "        lines = f.readlines()\n",
        "# Sorting entities alphabetically\n",
        "documentRepo = OrderedDict()\n",
        "for line in lines:\n",
        "    s = line.split('\\t')\n",
        "    entity = s[0]\n",
        "    def_id = s[1]\n",
        "    definition = s[2].replace('\\n','')\n",
        "    documentRepo[def_id] = [entity, definition]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiJrap2F08w5",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Word2Vec\n",
        "\n",
        "In this part the Word2Vec algorithm will be used to generate word embeddings for tokens in the dataset. You can just use a package like https://radimrehurek.com/gensim/models/word2vec.html. Let's set the size of word embeddings to be 20. Please print the word embeddings for the tokens: \n",
        "* relational\n",
        "* database\n",
        "* garbage\n",
        "* collection\n",
        "* retrieval \n",
        "* model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f773Vy2dAeA",
        "colab_type": "code",
        "outputId": "fbaf5d22-55be-4297-8462-20b337b4fc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# code here.\n",
        "# how do you generate the word embeddings\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "import gensim \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def read_input(input_file):\n",
        "    documents = []\n",
        "    with open(input_file, encoding='utf8') as f:\n",
        "        filtered_line = \"\"\n",
        "        for line in f:\n",
        "            line = re.sub(r'[:=*?`@%+!&\";#{}\\[\\]\\'^|\\~]','', line.split('\\t')[2].replace('\\n',''))\n",
        "            line = re.sub(r'[,-\\\\—()\\.<>]', ' ', line) #replace seperating punctuations with space to be split later            \n",
        "            line = re.sub(r'[^A-Za-z0-9❖•]', ' ', line)\n",
        "            line = re.sub(r'[❖•]', ' ', line)\n",
        "            line = ' '.join([porter.stem(word) for word in line.split() if word not in set(stopwords.words(\"english\"))])\n",
        "            yield gensim.utils.simple_preprocess (line)\n",
        "    return documents\n",
        "# read the tokenized reviews into a list\n",
        "# each review item becomes a serries of words\n",
        "# so this becomes a list of lists\n",
        "documents = list (read_input ('homework_1_data.txt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDWSoTDmScm0",
        "colab_type": "code",
        "outputId": "760bb856-5ebf-40e4-b6ad-5dafda128fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "porter = PorterStemmer()\n",
        "from collections import OrderedDict\n",
        "with open('homework_1_data.txt', encoding='utf8') as f:\n",
        "        lines = f.readlines()\n",
        "stemDocumentRepo = OrderedDict()\n",
        "for line in lines:\n",
        "    s = line.split('\\t')\n",
        "    entity = s[0]\n",
        "    def_id = s[1]\n",
        "    line = re.sub(r'[:=*?`@%+!&\";#{}\\[\\]\\'^|\\~]','', line.split('\\t')[2].replace('\\n',''))\n",
        "    line = re.sub(r'[,-\\\\—()\\.<>]', ' ', line) #replace seperating punctuations with space to be split later            \n",
        "    line = re.sub(r'[^A-Za-z0-9❖•]', ' ', line)\n",
        "    line = re.sub(r'[❖•]', ' ', line)\n",
        "    line = ' '.join([porter.stem(word) for word in line.split() if word not in stopwords.words(\"english\")])\n",
        "    definition = line\n",
        "    stemDocumentRepo[def_id] = [entity, definition]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt0rAoQmHzYI",
        "colab_type": "code",
        "outputId": "cfaa71dd-1a41-4001-f670-e1aa386ce438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(list(documents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtAE-JnKgYfv",
        "colab_type": "code",
        "outputId": "68a170aa-04bb-489d-cbfa-7d5ddd4fd205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model = gensim.models.Word2Vec (documents, size=20, window=10, min_count=2, workers=10)\n",
        "model.train(documents, total_examples=len(documents), epochs=30)\n",
        "model.save(\"word2vec_hw4.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxTg3RPygbH_",
        "colab_type": "code",
        "outputId": "2032c8e7-29a8-40ab-da26-637058131165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"word2vec_hw4.model\")\n",
        "#https://github.com/kavgan/nlp-in-practice/blob/master/word2vec/Word2Vec.ipynb\n",
        "queries = \"relational database garbage collection retrieval model\"\n",
        "for query in queries.split():\n",
        "    print (\"Original Query: \" + str(query))\n",
        "    print (\"Parsed Query: \" + str(porter.stem(query)))\n",
        "    print (model.wv[porter.stem(query)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Query: relational\n",
            "Parsed Query: relat\n",
            "[ 2.4325955  -1.5862104   0.9993653  -2.3630714  -3.6648803  -1.400936\n",
            " -1.5606936  -0.12180549  2.5645795  -1.4470593   0.6036795  -1.8059071\n",
            " -0.59320927  0.56289136  5.02749     4.4252834   2.4621477  -2.636864\n",
            "  4.0297747   0.17841956]\n",
            "Original Query: database\n",
            "Parsed Query: databas\n",
            "[ 3.2639737  -0.40196422  1.2192895  -0.67061245  0.30400693  2.2527425\n",
            " -3.38287    -1.1784745   0.04066178 -4.403574    2.075228   -0.26188228\n",
            " -2.9757793  -0.04731051  5.0337152   3.7241054   1.0659866  -4.2606587\n",
            "  1.4258126  -3.2073882 ]\n",
            "Original Query: garbage\n",
            "Parsed Query: garbag\n",
            "[ 0.56764954  0.4797237   0.3522163   0.14468074  1.0309417  -0.21139057\n",
            " -1.0061702  -1.1167006  -1.0311371  -0.58094466  0.27873987  1.4679729\n",
            "  0.08956432  2.0702474  -0.6433034  -0.29034895  2.1073434   1.1227099\n",
            "  1.588469    0.16232611]\n",
            "Original Query: collection\n",
            "Parsed Query: collect\n",
            "[ 3.2466333   2.0754826   0.8773468  -1.1259906   1.869884    3.6180139\n",
            " -2.4340394  -1.3119506   2.6132262  -0.49490678  0.8424224  -0.23356353\n",
            " -0.34636554 -1.3903335   5.9803433   2.5191312   1.3619741  -1.4629959\n",
            "  1.1435415  -1.2641444 ]\n",
            "Original Query: retrieval\n",
            "Parsed Query: retriev\n",
            "[ 3.9590335  -0.6342351  -1.4023343  -2.4964745   4.099001    4.024918\n",
            " -2.1609597   0.09994639 -0.52685755 -4.567739    2.0032625   1.8901532\n",
            " -0.08855495 -0.7616884   2.195657    3.9000664   2.4730196   0.0163672\n",
            " -1.9483706  -2.9267335 ]\n",
            "Original Query: model\n",
            "Parsed Query: model\n",
            "[-2.8467402  -1.4230542  -0.4411528  -3.7893302  -1.1716305  -1.6187179\n",
            "  0.78067887  1.4126737  -1.1742904  -1.7177204  -0.23752119 -0.20162924\n",
            " -1.9023662  -0.14522131  2.5893087  -0.3378651  -1.8762882  -6.3016553\n",
            "  0.813192    1.1572758 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O0nwngL08xc",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Vector Space Model via Word Embeddings\n",
        "\n",
        "In this part, the job is to match the query and the document via the cosine similarity between the embeddings of them.\n",
        "\n",
        "Since there are not just one token in a query or a document, the first challenge is how to aggregate many word embeddings into one embedding of a query or a document. There are many ways to do so: \n",
        "* Max pooling: return the maximum value along each dimension of a bunch of word embeddings. For example, [1, 3, 4], [2, 1, 5] -> [2, 3, 5].\n",
        "* Min pooling: return the minimum value along each dimension of a bunch of word embeddings\n",
        "* Mean pooling: return the mean value along each dimension of a bunch of word embeddings\n",
        "* Sum: element-wise add a bunch of word embeddings together\n",
        "* Weighted sum: assign weights to word embeddings and then add them together. Weights could be TF, IDF or TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8IccDV5Am3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "\n",
        "def vector_pooling(querys, definition, model, method):\n",
        "    # query_vec # query vector\n",
        "    # doc_vec # document vector\n",
        "    query_vec = [] * 20\n",
        "    doc_vec = [] * 20\n",
        "    if method == \"max\":\n",
        "        query_vec = [-sys.maxsize] * 20\n",
        "        doc_vec = [-sys.maxsize] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.maximum(np.array(query_vec), model.wv[porter.stem(query)])\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec   = np.maximum(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"min\":\n",
        "        query_vec = [sys.maxsize] * 20\n",
        "        doc_vec = [sys.maxsize] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.minimum(np.array(query_vec), model.wv[porter.stem(query)])\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec   = np.minimum(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"mean\":\n",
        "        query_list = [0] * 20\n",
        "        count_q = len(querys.split())\n",
        "        for query in querys.split():\n",
        "            query_list = np.add(query_list, model.wv[porter.stem(query)])\n",
        "        query_vec = [x/count_q for x in query_list]\n",
        "\n",
        "        doc_list = [0] * 20\n",
        "        count_d = len(definition.split())\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_list = np.add(doc_list, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "        doc_vec = [x/count_q for x in doc_list]\n",
        "\n",
        "    elif method == \"sum\":\n",
        "        query_vec = [0] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.add(query_vec, model.wv[porter.stem(query)])\n",
        "\n",
        "        doc_vec = [0] * 20\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec = np.add(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"weight\":\n",
        "        query_list = [0] * 20\n",
        "        term = []\n",
        "        count_q = len(querys.split())\n",
        "        for query in querys.split():\n",
        "            term.append(porter.stem(query))\n",
        "            query_list = np.add(query_list, model.wv[porter.stem(query)])\n",
        "        query_vec = [x/count_q for x in query_list]\n",
        "\n",
        "        count_d = len(definition.split())\n",
        "        tf = 1\n",
        "        doc_list = [0] * 20\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                if word in term:\n",
        "                    if definition.count(word) > 0:\n",
        "                        tf = 1 + math.log10(definition.count(word))\n",
        "                    doc_list = np.add(doc_list, tf * model.wv[word])\n",
        "                else:\n",
        "                    doc_list = np.add(doc_list, 0.05 * model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "        if count_d == 0:\n",
        "            doc_vec = [0] * 20\n",
        "        else:\n",
        "            doc_vec = [x/count_d for x in doc_list]\n",
        "\n",
        "    return query_vec, doc_vec\n",
        "\n",
        "def vector_space_model_with_embeddings (querys, definition, model, method):\n",
        "    # return the cosine similarity of query_vec and doc_vec\n",
        "    query_vec, doc_vec = vector_pooling(querys, definition, model, method)\n",
        "    dot_product = sum(i[0] * i[1] for i in zip(query_vec, doc_vec))\n",
        "    if np.linalg.norm(doc_vec) == 0:\n",
        "        cosine = -1\n",
        "    else:\n",
        "        cosine = dot_product / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec))\n",
        "    return cosine\n",
        "\n",
        "def rank_querys_docs (stemDocumentRepo, querys, model):\n",
        "    maxScores = {}\n",
        "    minScores = {}\n",
        "    meanScores = {}\n",
        "    sumScores = {}\n",
        "    weightScores = {}\n",
        "    for i in range (len(stemDocumentRepo)):\n",
        "        _, stemdefinition = stemDocumentRepo[str(i)]\n",
        "        # Max method\n",
        "        maxScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"max\")\n",
        "        # Max method\n",
        "        minScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"min\")\n",
        "        # Mean method\n",
        "        meanScores[i] = vector_space_model_with_embeddings(querys, stemdefinition, model, \"mean\")\n",
        "        # Sum method\n",
        "        sumScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"sum\")\n",
        "        # TF weighting method\n",
        "        weightScores[i] = vector_space_model_with_embeddings(querys, stemdefinition, model, \"weight\")\n",
        "    return maxScores, minScores, meanScores, sumScores, weightScores\n",
        "\n",
        "def calculate_precision10 (docScores, query, type):\n",
        "    printTopN = 10\n",
        "    count = 0\n",
        "    #print (\"#######\" + str(type) + \"#######\")\n",
        "    for docID, score in sorted(docScores.items(), key=lambda item: item[1], reverse = True):\n",
        "        #print (str(documentRepo[str(docID)][0]) + \" \"+ str(score))\n",
        "        if printTopN > 0:\n",
        "            if query == documentRepo[str(docID)][0]:\n",
        "                count = count + 1\n",
        "            printTopN-=1\n",
        "        else:\n",
        "            break\n",
        "    return count/10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8NWqvOU08xe",
        "colab_type": "code",
        "outputId": "7f2945de-49a5-422b-cef4-192874500d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# your code here\n",
        "query = \"relational database\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query, model)\n",
        "print (\"Query: \" + str(query))\n",
        "print (\"Precision@10: \")\n",
        "maxprecision = calculate_precision10(maxScores, query, \"max\")\n",
        "minprecision = calculate_precision10(minScores, query, \"min\")\n",
        "meanprecision = calculate_precision10(meanScores, query, \"mean\")\n",
        "sumprecision = calculate_precision10(sumScores, query, \"sum\")\n",
        "weightprecision = calculate_precision10(weightScores, query, \"weight\")\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum (TF): \" + str(weightprecision))\n",
        "\n",
        "query = \"garbage collection\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query, model)\n",
        "print (\"Query: \" + str(query))\n",
        "print (\"Precision@10: \")\n",
        "maxprecision = calculate_precision10(maxScores, query, \"max\")\n",
        "minprecision = calculate_precision10(minScores, query, \"min\")\n",
        "meanprecision = calculate_precision10(meanScores, query, \"mean\")\n",
        "sumprecision = calculate_precision10(sumScores, query, \"sum\")\n",
        "weightprecision = calculate_precision10(weightScores, query, \"weight\")\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum (TF): \" + str(weightprecision))\n",
        "\n",
        "query = \"retrieval model\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query, model)\n",
        "print (\"Query: \" + str(query))\n",
        "print (\"Precision@10: \")\n",
        "maxprecision = calculate_precision10(maxScores, query, \"max\")\n",
        "minprecision = calculate_precision10(minScores, query, \"min\")\n",
        "meanprecision = calculate_precision10(meanScores, query, \"mean\")\n",
        "sumprecision = calculate_precision10(sumScores, query, \"sum\")\n",
        "weightprecision = calculate_precision10(weightScores, query, \"weight\")\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum (TF): \" + str(weightprecision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: relational database\n",
            "Precision@10: \n",
            "Max pooling: 0.4\n",
            "Min pooling: 0.6\n",
            "Mean pooling: 0.4\n",
            "Sum pooling: 0.4\n",
            "Weighted sum (TF): 0.5\n",
            "Query: garbage collection\n",
            "Precision@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum (TF): 0.5\n",
            "Query: retrieval model\n",
            "Precision@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum (TF): 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFuuTRQl08xo",
        "colab_type": "text"
      },
      "source": [
        "Try different aggregation methods and report the precision@10 for these queries:\n",
        "* query: relational database\n",
        "* query: garbage collection\n",
        "* query: retrieval model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr4DSh2508xq",
        "colab_type": "text"
      },
      "source": [
        "### Discussion\n",
        "Among these aggregation methods, which one is the best and which one is the worst?\n",
        "<br>**[ANS]**\n",
        "<br>Based on the experimental results of precision@10, the best method is the **Weighted sum (TF)**, the worst method is the **max,mean,sum** method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWfNX-__08xs",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Query Expansion via Word Embeddings\n",
        "\n",
        "Remember the hardest query \"retrieval model\" in homework 1? Because there is no document containing \"retrieval model\" in the dataset, you cannot retrieve any documents by Boolean matching. Now, it is the time of your \"revenge\" via query expansion.\n",
        "\n",
        "In this part, your job is to expand the original query like \"retrieval model\" by adding semantically similar words (e.g., \"search\"), which are selected from all tokens in the dataset.\n",
        "\n",
        "There are many ways to do so. For this part, we want you to calculate the cosine similarity between each of the original query tokens and the other tokens based on their word embeddings.\n",
        "\n",
        "First, please find the top 3 similar tokens for:\n",
        "* relational\n",
        "* database\n",
        "* garbage\n",
        "* collection\n",
        "* retrieval \n",
        "* model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlojBC_z4QD8",
        "colab_type": "code",
        "outputId": "661a1cdc-df74-4c3a-890c-9249ae25367c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "queries = \"relational database garbage collection retrieval model\"\n",
        "# Calculate the relevant term for each query\n",
        "query1 = \"relational database\"\n",
        "query2 = \"garbage collection\"\n",
        "query3 = \"retrieval model\"\n",
        "relevant1 = 0\n",
        "relevant2 = 0\n",
        "relevant3 = 0\n",
        "\n",
        "for i in range (len(stemDocumentRepo)):\n",
        "        entity, _ = documentRepo[str(i)]\n",
        "        if entity == query1:\n",
        "            relevant1 += 1\n",
        "        elif entity == query2:\n",
        "            relevant2 += 1\n",
        "        elif entity == query3:\n",
        "            relevant3 += 1\n",
        "print (\"The number of \" + str(query1) + \": \" + str(relevant1))\n",
        "print (\"The number of \" + str(query2) + \": \" + str(relevant2))\n",
        "print (\"The number of \" + str(query3) + \": \" + str(relevant3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of relational database: 284\n",
            "The number of garbage collection: 38\n",
            "The number of retrieval model: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EypG_JAer7kW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "        \n",
        "def vector_pooling(querys, definition, model, method):\n",
        "    # query_vec # query vector\n",
        "    # doc_vec # document vector\n",
        "    query_vec = [] * 20\n",
        "    doc_vec = [] * 20\n",
        "    if method == \"max\":\n",
        "        query_vec = [-sys.maxsize] * 20\n",
        "        doc_vec = [-sys.maxsize] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.maximum(np.array(query_vec), model.wv[query])\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec   = np.maximum(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"min\":\n",
        "        query_vec = [sys.maxsize] * 20\n",
        "        doc_vec = [sys.maxsize] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.minimum(np.array(query_vec), model.wv[query])\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec   = np.minimum(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"mean\":\n",
        "        query_list = [0] * 20\n",
        "        count_q = len(querys.split())\n",
        "        for query in querys.split():\n",
        "            query_list = np.add(query_list, model.wv[query])\n",
        "        query_vec = [x/count_q for x in query_list]\n",
        "\n",
        "        doc_list = [0] * 20\n",
        "        count_d = len(definition.split())\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_list = np.add(doc_list, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "        doc_vec = [x/count_q for x in doc_list]\n",
        "\n",
        "    elif method == \"sum\":\n",
        "        query_vec = [0] * 20\n",
        "        for query in querys.split():\n",
        "            query_vec = np.add(query_vec, model.wv[query])\n",
        "\n",
        "        doc_vec = [0] * 20\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                doc_vec = np.add(doc_vec, model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    elif method == \"weight\":\n",
        "        query_list = [0] * 20\n",
        "        term1 = [] # original query\n",
        "        term2 = [] # expanded query\n",
        "        idx = 0\n",
        "        count_q = 4 # original 2, expanded 6 * 1/3 = 2\n",
        "        for query in querys.split():\n",
        "            idx = idx + 1\n",
        "            if idx <= 2:\n",
        "                term1.append(query)\n",
        "                query_list = np.add(query_list, model.wv[query])\n",
        "            else:\n",
        "                term2.append(query)\n",
        "                query_list = np.add(query_list, model.wv[query]/3)\n",
        "        query_vec = [x/count_q for x in query_list]\n",
        "\n",
        "        count_d = len(definition.split())\n",
        "        tf = 1\n",
        "        doc_list = [0] * 20\n",
        "        for word in definition.split():\n",
        "            try:\n",
        "                if word in term1:\n",
        "                    if definition.count(word) > 0:\n",
        "                        tf = 1 + math.log10(definition.count(word))\n",
        "                    doc_list = np.add(doc_list, tf * model.wv[word])\n",
        "                elif word in term2:\n",
        "                    if definition.count(word) > 0:\n",
        "                        tf = 1 + math.log10(definition.count(word))\n",
        "                    doc_list = np.add(doc_list, tf/3 * model.wv[word])\n",
        "                else:\n",
        "                    doc_list = np.add(doc_list, 0.05 * model.wv[word])\n",
        "            except KeyError:\n",
        "                continue\n",
        "        if count_d == 0:\n",
        "            doc_vec = [0] * 20\n",
        "        else:\n",
        "            doc_vec = [x/count_d for x in doc_list]\n",
        "\n",
        "    return query_vec, doc_vec\n",
        "\n",
        "def vector_space_model_with_embeddings (querys, definition, model, method):\n",
        "    # return the cosine similarity of query_vec and doc_vec\n",
        "    query_vec, doc_vec = vector_pooling(querys, definition, model, method)\n",
        "    dot_product = sum(i[0] * i[1] for i in zip(query_vec, doc_vec))\n",
        "    if np.linalg.norm(doc_vec) == 0:\n",
        "        cosine = -1\n",
        "    else:\n",
        "        cosine = dot_product / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec))\n",
        "    return cosine\n",
        "\n",
        "def rank_querys_docs (stemDocumentRepo, querys, model):\n",
        "    maxScores = {}\n",
        "    minScores = {}\n",
        "    meanScores = {}\n",
        "    sumScores = {}\n",
        "    weightScores = {}\n",
        "    for i in range (len(stemDocumentRepo)):\n",
        "        _, stemdefinition = stemDocumentRepo[str(i)]\n",
        "        entity, _ = documentRepo[str(i)]\n",
        "        # Max method\n",
        "        maxScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"max\")\n",
        "        # Max method\n",
        "        minScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"min\")\n",
        "        # Mean method\n",
        "        meanScores[i] = vector_space_model_with_embeddings(querys, stemdefinition, model, \"mean\")\n",
        "        # Sum method\n",
        "        sumScores[i]  = vector_space_model_with_embeddings(querys, stemdefinition, model, \"sum\")\n",
        "        # TF weighting method\n",
        "        weightScores[i] = vector_space_model_with_embeddings(querys, stemdefinition, model, \"weight\")\n",
        "    return maxScores, minScores, meanScores, sumScores, weightScores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWXM0n5108xv",
        "colab_type": "code",
        "outputId": "dfca0738-837f-43d4-bd2d-c0081495836e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# your code here\n",
        "import warnings  \n",
        "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "model = Word2Vec.load(\"word2vec_hw4.model\")\n",
        "tokens = \"relational database garbage collection retrieval model\"\n",
        "\n",
        "print(model.wv.most_similar(positive=porter.stem('relational'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('database'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('garbage'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('collection'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('retrieval'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('model'), topn=3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('tabl', 0.8046892881393433), ('entiti', 0.7562270164489746), ('common', 0.7351295351982117)]\n",
            "[('dbm', 0.8211696147918701), ('collect', 0.7607239484786987), ('compris', 0.7468408942222595)]\n",
            "[('collector', 0.8240309953689575), ('longer', 0.8046669960021973), ('reclaim', 0.7889659404754639)]\n",
            "[('repositori', 0.7911487817764282), ('data', 0.7804063558578491), ('gather', 0.7676119208335876)]\n",
            "[('store', 0.873002827167511), ('data', 0.7816286087036133), ('metadata', 0.7605143785476685)]\n",
            "[('mathemat', 0.787505030632019), ('breakthrough', 0.7493785619735718), ('illustr', 0.7133469581604004)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhuZLanM08x3",
        "colab_type": "text"
      },
      "source": [
        "Second, please add these similar tokens to the orignal query and redo the **vector space model** in part 2. \n",
        "* query: relational database\n",
        "* query: garbage collection\n",
        "* query: retrieval model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_x0Liga08x5",
        "colab_type": "code",
        "outputId": "d6bf99ac-046c-44db-971a-97644ae89bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# your code here\n",
        "def calculate_recall_10 (docScores, query, total_relevants):\n",
        "    printTopN = 10\n",
        "    count = 0\n",
        "    #print (\"##############\")\n",
        "    for docID, score in sorted(docScores.items(), key=lambda item: item[1], reverse = True):\n",
        "        if printTopN > 0:\n",
        "            #print (str(documentRepo[str(docID)][0]) + \" \"+ str(score))\n",
        "            if query == documentRepo[str(docID)][0]:\n",
        "                count = count + 1\n",
        "            printTopN-=1\n",
        "        else:\n",
        "            break\n",
        "    return count/total_relevants\n",
        "\n",
        "query1 = \"relational database\"\n",
        "query1_stem = \"relat databas\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query1_stem, model)\n",
        "print (\"Query: \" + str(query1))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query1, relevant1)\n",
        "minprecision = calculate_recall_10(minScores, query1, relevant1)\n",
        "meanprecision = calculate_recall_10(meanScores, query1, relevant1)\n",
        "sumprecision = calculate_recall_10(sumScores, query1, relevant1)\n",
        "weightprecision = calculate_recall_10(weightScores, query1, relevant1)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))\n",
        "\n",
        "query2 = \"garbage collection\"\n",
        "query2_stem = \"garbag collect\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query2_stem, model)\n",
        "print (\"Query: \" + str(query2))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query2, relevant2)\n",
        "minprecision = calculate_recall_10(minScores, query2, relevant2)\n",
        "meanprecision = calculate_recall_10(meanScores, query2, relevant2)\n",
        "sumprecision = calculate_recall_10(sumScores, query2, relevant2)\n",
        "weightprecision = calculate_recall_10(weightScores, query2, relevant2)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))\n",
        "\n",
        "query3 = \"retrieval model\"\n",
        "query3_stem = \"retriev model\"\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query3_stem, model)\n",
        "print (\"Query: \" + str(query3))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query3, relevant3)\n",
        "minprecision = calculate_recall_10(minScores, query3, relevant3)\n",
        "meanprecision = calculate_recall_10(meanScores, query3, relevant3)\n",
        "sumprecision = calculate_recall_10(sumScores, query3, relevant3)\n",
        "weightprecision = calculate_recall_10(weightScores, query3, relevant3)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: relational database\n",
            "Recall@10: \n",
            "Max pooling: 0.014084507042253521\n",
            "Min pooling: 0.02112676056338028\n",
            "Mean pooling: 0.014084507042253521\n",
            "Sum pooling: 0.014084507042253521\n",
            "Weighted sum: 0.017605633802816902\n",
            "Query: garbage collection\n",
            "Recall@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum: 0.13157894736842105\n",
            "Query: retrieval model\n",
            "Recall@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwJyGgF08x_",
        "colab_type": "text"
      },
      "source": [
        "Report recall@10 before the query expansion:\n",
        "<br>**[ANS] As above**\n",
        "<br>**Query: relational database**\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.014084507042253521\n",
        "<br>Min pooling: 0.02112676056338028\n",
        "<br>Mean pooling: 0.014084507042253521\n",
        "<br>Sum pooling: 0.014084507042253521\n",
        "<br>Weighted sum: 0.017605633802816902\n",
        "<br>\n",
        "<br>**Query: garbage collection**\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.0\n",
        "<br>Min pooling: 0.0\n",
        "<br>Mean pooling: 0.0\n",
        "<br>Sum pooling: 0.0\n",
        "<br>Weighted sum: 0.13157894736842105\n",
        "<br>\n",
        "<br>**Query: retrieval model**\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.0\n",
        "<br>Min pooling: 0.0\n",
        "<br>Mean pooling: 0.0\n",
        "<br>Sum pooling: 0.0\n",
        "<br>Weighted sum: 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2jPnKRY08yA",
        "colab_type": "text"
      },
      "source": [
        "Report recall@10 after the query expansion:\n",
        "<br>**[ANS] As below**\n",
        "<br>**Query: relational database**\n",
        "<br>Query Expansion:  relat tabl entiti common databas dbm collect compris\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.014084507042253521\n",
        "<br>Min pooling: 0.028169014084507043\n",
        "<br>Mean pooling: 0.028169014084507043\n",
        "<br>um pooling: 0.028169014084507043\n",
        "<br>Weighted sum: 0.028169014084507043\n",
        "<br>\n",
        "<br>**Query: garbage collection**\n",
        "<br>Query Expansion:  garbag collector longer reclaim collect repositori data gather\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.0\n",
        "<br>Min pooling: 0.0\n",
        "<br>Mean pooling: 0.0\n",
        "<br>Sum pooling: 0.0\n",
        "<br>Weighted sum: 0.13157894736842105\n",
        "<br>\n",
        "<br>**Query: retrieval model**\n",
        "<br>Query Expansion:  retriev store data metadata model mathemat breakthrough illustr\n",
        "<br>Recall@10: \n",
        "<br>Max pooling: 0.0\n",
        "<br>Min pooling: 0.0\n",
        "<br>Mean pooling: 0.0\n",
        "<br>Sum pooling: 0.0\n",
        "<br>Weighted sum: 0.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qt6Oh_44kZB",
        "colab_type": "code",
        "outputId": "aafef171-6c44-4175-9f48-0d677919e4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(model.wv.most_similar(positive=porter.stem('relational'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('database'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('garbage'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('collection'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('retrieval'), topn=3))\n",
        "print(model.wv.most_similar(positive=porter.stem('model'), topn=3))\n",
        "\n",
        "def query_expansion(querys):\n",
        "    querys_expand = ''\n",
        "    for query in querys.split():\n",
        "        querys_expand = querys_expand + \" \" + porter.stem(query)\n",
        "        querys_expand = querys_expand + \" \" + str(model.wv.most_similar(positive=porter.stem(query), topn=3)[0][0])\n",
        "        querys_expand = querys_expand + \" \" + str(model.wv.most_similar(positive=porter.stem(query), topn=3)[1][0])\n",
        "        querys_expand = querys_expand + \" \" + str(model.wv.most_similar(positive=porter.stem(query), topn=3)[2][0])\n",
        "    return querys_expand"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('tabl', 0.8046892881393433), ('entiti', 0.7562270164489746), ('common', 0.7351295351982117)]\n",
            "[('dbm', 0.8211696147918701), ('collect', 0.7607239484786987), ('compris', 0.7468408942222595)]\n",
            "[('collector', 0.8240309953689575), ('longer', 0.8046669960021973), ('reclaim', 0.7889659404754639)]\n",
            "[('repositori', 0.7911487817764282), ('data', 0.7804063558578491), ('gather', 0.7676119208335876)]\n",
            "[('store', 0.873002827167511), ('data', 0.7816286087036133), ('metadata', 0.7605143785476685)]\n",
            "[('mathemat', 0.787505030632019), ('breakthrough', 0.7493785619735718), ('illustr', 0.7133469581604004)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1KY6QbrG5AW",
        "colab_type": "code",
        "outputId": "c0145174-ee74-4aad-801d-7c8263736347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "query1 = \"relational database\"\n",
        "query1_expand = query_expansion(query1)\n",
        "query2 = \"garbage collection\"\n",
        "query2_expand = query_expansion(query2)\n",
        "query3 = \"retrieval model\"\n",
        "query3_expand = query_expansion(query3)\n",
        "\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query1_expand, model)\n",
        "print (\"Query: \" + str(query1))\n",
        "print (\"Query Expansion: \" + str(query1_expand))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query1, relevant1)\n",
        "minprecision = calculate_recall_10(minScores, query1, relevant1)\n",
        "meanprecision = calculate_recall_10(meanScores, query1, relevant1)\n",
        "sumprecision = calculate_recall_10(sumScores, query1, relevant1)\n",
        "weightprecision = calculate_recall_10(weightScores, query1, relevant1)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))\n",
        "\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query2_expand, model)\n",
        "print (\"Query: \" + str(query2))\n",
        "print (\"Query Expansion: \" + str(query2_expand))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query2, relevant2)\n",
        "minprecision = calculate_recall_10(minScores, query2, relevant2)\n",
        "meanprecision = calculate_recall_10(meanScores, query2, relevant2)\n",
        "sumprecision = calculate_recall_10(sumScores, query2, relevant2)\n",
        "weightprecision = calculate_recall_10(weightScores, query2, relevant2)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))\n",
        "\n",
        "maxScores, minScores, meanScores, sumScores, weightScores = rank_querys_docs (stemDocumentRepo, query3_expand, model)\n",
        "print (\"Query: \" + str(query3))\n",
        "print (\"Query Expansion: \" + str(query3_expand))\n",
        "print (\"Recall@10: \")\n",
        "maxprecision = calculate_recall_10(maxScores, query3, relevant3)\n",
        "minprecision = calculate_recall_10(minScores, query3, relevant3)\n",
        "meanprecision = calculate_recall_10(meanScores, query3, relevant3)\n",
        "sumprecision = calculate_recall_10(sumScores, query3, relevant3)\n",
        "weightprecision = calculate_recall_10(weightScores, query3, relevant3)\n",
        "print (\"Max pooling: \" + str(maxprecision))\n",
        "print (\"Min pooling: \" + str(minprecision))\n",
        "print (\"Mean pooling: \" + str(meanprecision))\n",
        "print (\"Sum pooling: \" + str(sumprecision))\n",
        "print (\"Weighted sum: \" + str(weightprecision))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Query: relational database\n",
            "Query Expansion:  relat tabl entiti common databas dbm collect compris\n",
            "Recall@10: \n",
            "Max pooling: 0.014084507042253521\n",
            "Min pooling: 0.028169014084507043\n",
            "Mean pooling: 0.028169014084507043\n",
            "Sum pooling: 0.028169014084507043\n",
            "Weighted sum: 0.028169014084507043\n",
            "Query: garbage collection\n",
            "Query Expansion:  garbag collector longer reclaim collect repositori data gather\n",
            "Recall@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum: 0.13157894736842105\n",
            "Query: retrieval model\n",
            "Query Expansion:  retriev store data metadata model mathemat breakthrough illustr\n",
            "Recall@10: \n",
            "Max pooling: 0.0\n",
            "Min pooling: 0.0\n",
            "Mean pooling: 0.0\n",
            "Sum pooling: 0.0\n",
            "Weighted sum: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsB85vru08yB",
        "colab_type": "text"
      },
      "source": [
        "### Discussions\n",
        "Why we measure recall here instead of precision or NDCG?\n",
        "<br>**[ANS]**\n",
        "**The Query Expansion may significantly decrease precision/NDCG rate, particularly with ambiguous terms. But we can extract as much possibilities of the correct term. Therefore, we select recall metric here**\n",
        "\n",
        "Should the tokens added for expansion have the same importance as the original query tokens? If not, how to improve the query expansion in this part?\n",
        "<br>**[ANS]**\n",
        "<br>**No, I assign lower weightings to the expanded queries: if the original query's weighting is 1.0, the one for expanded query is 0.33333**\n",
        "<br>**My guessed suggestion here is, maybe we discard the combination based method, but get top 10 results of each query and their expanded queries, then do intersections.**\n",
        "<br>\n",
        "<br>For instance, take the following example.\n",
        "<br>Query: relational database\n",
        "<br>**Query Expansion: relat tabl entiti common databas dbm collect compris**\n",
        "<br>We can split by [relat databas], [tabl dbm], [entiti, collect], [common, compris] and find those query-pairs' top 10, and implement intersections.\n",
        "<br>Also, if needed, we can add higher weightings for the original query, which is [relat databas]."
      ]
    }
  ]
}